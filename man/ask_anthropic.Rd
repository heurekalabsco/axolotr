% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fun.R
\name{ask_anthropic}
\alias{ask_anthropic}
\title{Call Anthropic API with Optional Prompt Caching}
\usage{
ask_anthropic(
  prompt,
  system = NULL,
  model = "claude",
  temperature = 0,
  max_tokens = 4096,
  pre_fill = NULL,
  pdf_path = NULL,
  cache_system = FALSE,
  cache_pdf = FALSE,
  dev = FALSE,
  ...
)
}
\arguments{
\item{prompt}{A character string representing the prompt to be sent to the Claude API.}

\item{system}{An optional character string to provide a system prompt to the model. Default is NULL.}

\item{model}{A character string specifying the model to use.}

\item{temperature}{A numeric value representing the temperature parameter. Default is 0.}

\item{max_tokens}{An integer specifying the maximum number of tokens in the response. Default is 4096.}

\item{pre_fill}{An optional character string to pre-fill the model's response. Default is NULL.}

\item{pdf_path}{Optional path to a PDF file.}

\item{cache_system}{Logical indicating whether to cache the system prompt. Default is FALSE. This content will be cached for 5 minutes and can be reused across multiple calls.}

\item{cache_pdf}{Logical indicating whether to cache the PDF content. Default is FALSE. This content will be cached for 5 minutes and can be reused across multiple calls.}

\item{dev}{Logical indicating whether to return the full response from the API. Default is FALSE.}

\item{...}{Additional parameters to pass to the Anthropic API.}
}
\value{
The generated text response from the Claude API, or NULL if an error occurs.
}
\description{
This function sends a prompt to the Claude API by Anthropic and returns the generated text response.
It supports prompt caching for improved performance when using consistent background context.
}
